{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Lecture Mathematical Foundations\n",
    "Horacio Gomez-Acevedo\n",
    "\n",
    "BMIG 6201\n",
    "\n",
    "UAMS\n",
    "\n",
    "\n",
    "# Basic Concepts of Linear Algebra\n",
    "\n",
    "Mathematical quantities are normally associated with sets of numbers. For instance points on the plane have two coordinates $x$ and $y$, whereas on the space they will have three coordinates $x$,$y$ and $z$. We further extend this idea to say that $(x_1,\\ldots,x_n)$ is a point (or vector) in the $n$-dimensional space (normally denoted as $\\mathbb{R}^n)$. We will denote vectors by bold face symbols, for instance $\\pmb{x}=(x_1,\\ldots,x_n)$ \n",
    "\n",
    "Some basic operations on vectors are:\n",
    "\n",
    "\\begin{align*}\n",
    "\\pmb{x}+\\pmb{y}&=\n",
    "(x_1,x_2,\\ldots,x_n)+(y_1,y_2,\\ldots,y_n)=(x_1+y_1,x_2+y_2,\\ldots,x_n+y_n)\\\\\n",
    "\\pmb{x}-\\pmb{y}&=\n",
    "(x_1,x_2,\\ldots,x_n)-(y_1,y_2,\\ldots,y_n)=(x_1-y_1,x_2-y_2,\\ldots,x_n-y_n)\\\\\n",
    "\\alpha \\pmb{x} &=\\alpha (x_1,x_2,\\ldots,x_n)= (\\alpha x_1,\\alpha x_2, \\ldots, \\alpha x_n) \\quad \\alpha \\in \\mathbb{R}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "The **dot product** of two vectors $\\pmb{x}$ and $\\pmb{y}$ is defined as\n",
    "\n",
    "\\begin{align*}\n",
    "\\pmb{x}\\cdot\\pmb{y}&=(x_1,x_2,\\ldots,x_n)\\cdot(y_1,y_2,\\ldots,y_n)=x_1\\cdot y_1+ x_2\\cdot y_2,\\ldots+x_n\\cdot y_n\\\\\n",
    "&=\\sum_{i=1}^n x_iy_i\\\\\n",
    "\\end{align*}\n",
    "The length of an $n$-vector is given by\n",
    "\\begin{equation*}\n",
    "\\| \\pmb{x} \\| = \\sqrt{ \\pmb{x} \\cdot \\pmb{x}}  = \\sqrt{x_1^2+x_2^2+\\ldots+x_n^2}= \\sqrt{\\sum_{i=1}^n x_i^2}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "A matrix is a collection of numbers, arranged in a particular way, for instance\n",
    "\\begin{equation*}\n",
    "\\left( \n",
    "    \\matrix{1 & 2& 3\\\\ 4&5&6}\n",
    "    \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "We can think of a matrix as a square section of a table. \n",
    "A matrix of size $n\\times p$ is a table with $n$ rows and $p$ columns like\n",
    "\\begin{equation*}\n",
    "\\left( \n",
    "\\matrix{ x_{1,1} & x_{1,2} & \\ldots & x_{1,p} \\\\\n",
    "x_{2,1} & x_{2,2} & \\ldots & x_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,p}\n",
    "}\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "We will denote matrices with capital bold variables such as $\\pmb{X}$. \n",
    "\n",
    "Matrices can be represented as either column or row vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices operations \n",
    "\n",
    "The matrices have similar operations as vectors\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{X}+\\pmb{Y}= \\left( \n",
    "\\matrix{ x_{1,1} & x_{1,2} & \\ldots & x_{1,p} \\\\\n",
    "x_{2,1} & x_{2,2} & \\ldots & x_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,p}\n",
    "}\n",
    "\\right) +\n",
    "\\left( \n",
    "\\matrix{ y_{1,1} & y_{1,2} & \\ldots & y_{1,p} \\\\\n",
    "y_{2,1} & y_{2,2} & \\ldots & y_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y_{n,1} & y_{n,2} & \\cdots & y_{n,p}\n",
    "}\n",
    "\\right)= \n",
    "\\left( \n",
    "\\matrix{ x_{1,1}+y_{1,1} & x_{1,2}+ y_{1,2} & \\ldots & x_{1,p}+y_{1,p} \\\\\n",
    "x_{2,1}+y_{2,1} & x_{2,2}+y_{2,2} & \\ldots & x_{2,p} +y_{2,p}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1}+y_{n,1} & x_{n,2}+y_{n,2} & \\cdots & x_{n,p}+y_{n,p}\n",
    "}\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{X}-\\pmb{Y}= \\left( \n",
    "\\matrix{ x_{1,1} & x_{1,2} & \\ldots & x_{1,p} \\\\\n",
    "x_{2,1} & x_{2,2} & \\ldots & x_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,p}\n",
    "}\n",
    "\\right) -\n",
    "\\left( \n",
    "\\matrix{ y_{1,1} & y_{1,2} & \\ldots & y_{1,p} \\\\\n",
    "y_{2,1} & y_{2,2} & \\ldots & y_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y_{n,1} & y_{n,2} & \\cdots & y_{n,p}\n",
    "}\n",
    "\\right)= \n",
    "\\left( \n",
    "\\matrix{ x_{1,1}-y_{1,1} & x_{1,2}- y_{1,2} & \\ldots & x_{1,p}-y_{1,p} \\\\\n",
    "x_{2,1}-y_{2,1} & x_{2,2}-y_{2,2} & \\ldots & x_{2,p} -y_{2,p}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1}-y_{n,1} & x_{n,2}-y_{n,2} & \\cdots & x_{n,p}-y_{n,p}\n",
    "}\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "For any real number $\\alpha \\in \\mathbb{R}$\n",
    "\\begin{equation*}\n",
    "\\alpha \\pmb{X}= \\alpha\\left( \n",
    "\\matrix{ x_{1,1} & x_{1,2} & \\ldots & x_{1,p} \\\\\n",
    "x_{2,1} & x_{2,2} & \\ldots & x_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,p}\n",
    "}\n",
    "\\right)=\n",
    "\\left( \n",
    "\\matrix{\\alpha x_{1,1} & \\alpha x_{1,2} & \\ldots & \\alpha x_{1,p} \\\\\n",
    "\\alpha x_{2,1} & \\alpha x_{2,2} & \\ldots & \\alpha x_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\alpha x_{n,1} & \\alpha x_{n,2} & \\cdots & \\alpha x_{n,p}\n",
    "}\n",
    "\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transpose of a Matrix\n",
    "\n",
    "The transpose of a matrix is obtained by swapping rows and columns. \n",
    "\\begin{equation*}\n",
    "\\pmb{X}= \\left( \n",
    "\\matrix{ x_{1,1} & x_{1,2} & \\ldots & x_{1,p} \\\\\n",
    "x_{2,1} & x_{2,2} & \\ldots & x_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,p}\n",
    "}\n",
    "\\right) \n",
    "\\Rightarrow\n",
    "\\;\\pmb{X}^t=\n",
    " \\left( \n",
    "\\matrix{ x_{1,1} & x_{2,1} & \\ldots & x_{n,1} \\\\\n",
    "x_{1,2} & x_{2,2} & \\ldots & x_{n,2} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{1,p} & x_{2,p} & \\cdots & x_{n,p}\n",
    "}\n",
    "\\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix multiplication\n",
    "\n",
    "This operation is a bit different than the other operations. \n",
    "\n",
    "Let $\\pmb{X}$ be an $n\\times p$ matrix represented as an arrange of row vectors, and $\\pmb{Y}$ be a $p \\times k$ matrix represented as an arrange of column vectors\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{X}= \n",
    "\\left( \n",
    "\\matrix{ x_{1,1} & x_{1,2} & \\ldots & x_{1,p} \\\\\n",
    "x_{2,1} & x_{2,2} & \\ldots & x_{2,p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n,1} & x_{n,2} & \\cdots & x_{n,p}\n",
    "}\n",
    "\\right)=\n",
    "\\left( \n",
    "\\matrix{ \\pmb{x}_1  \\\\\n",
    "\\pmb{x}_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\pmb{x}_n\n",
    "}\n",
    "\\right) \\; \\textrm{and}\\;\n",
    "\\pmb{Y} = \n",
    "\\left( \n",
    "\\matrix{ y_{1,1} & y_{1,2} & \\ldots & y_{1,k} \\\\\n",
    "y_{2,1} & y_{2,2} & \\ldots & y_{2,k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y_{p,1} & y_{n,2} & \\cdots & y_{n,k}\n",
    "}\n",
    "\\right)=\n",
    "\\left(\n",
    "\\matrix{\\pmb{y}_1 & \\pmb{y}_2 & \\ldots & \\pmb{y}_k \\\\}\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "The multiplication of $\\pmb{X}$ and $\\pmb{Y}$ \n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{X}\\cdot\\pmb{Y}= \\left( \n",
    "\\matrix{ \\pmb{x}_1\\cdot\\pmb{y}_1  & \\pmb{x}_1\\cdot \\pmb{y}_2 & \\ldots & \\pmb{x}_1 \\cdot \\pmb{y}_k \\\\\n",
    "\\pmb{x}_2\\cdot\\pmb{y}_1  & \\pmb{x}_2\\cdot \\pmb{y}_2 & \\ldots & \\pmb{x}_2 \\cdot \\pmb{y}_k \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\pmb{x}_n\\cdot\\pmb{y}_1  & \\pmb{x}_n\\cdot \\pmb{y}_2 & \\ldots & \\pmb{x}_n \\cdot \\pmb{y}_k \\\\\n",
    "}\n",
    "\\right) \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "> Check the size of the matrix under multiplication! Multiplying a matrix of size $n\\times k$ by a matrix of size $k \\times p$ gives you a matrix of size $n \\times p$ (middle number $k$ must coincide but cancells out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "Consider the following matrices\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{A}=\\left(  \\matrix{2 & -5 \\\\ 1 & 2} \\right) \\; \\textrm{and } \\; \\pmb{B}= \\left( \\matrix{3 & 2 \\\\ -1 & 4} \\right) \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{A}\\cdot \\pmb{B}= \\left( \\matrix{ \n",
    "2*3 + (-5)*(-1) & 2*2+(-5)*4 \\\\\n",
    "1*3 + 2*(-1) & 1*2+2*4 }\n",
    "\\right)= \\left( \\matrix{11 & -16 \\\\ 1 & 10 } \\right)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Systems of Equations in Matrix Form\n",
    "\n",
    "Suppose you want to solve the following system of equations\n",
    "\n",
    "\\begin{align}\n",
    "2x -5y &= 3\\\\\n",
    "x + 2y &= 2\n",
    "\\end{align}\n",
    "\n",
    "At this point we write these equations as\n",
    "\n",
    "\\begin{equation}\n",
    "\\left(\n",
    "\\matrix{2 & -5\\\\ 1 & 2}\n",
    "\\right) \\cdot \\left(\n",
    "\\matrix{x \\\\ y} \n",
    "\\right)\n",
    "= \\left(\n",
    "\\matrix{3 \\\\ 2}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "If we call $A=\\left(\n",
    "\\matrix{2 & -5\\\\ 1 & 2}\n",
    "\\right)  $, $\\pmb{x}= \\left( \\matrix{x \\\\y} \\right) $ and $\\pmb{b}=\\left( \\matrix{3\\\\2}\\right)$, we can rewrite our system as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{A}\\cdot \\pmb{x}=\\pmb{b}\n",
    "\\end{equation*}\n",
    "\n",
    "Following our basic algebra knowledge, we would like to solve this equation as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{x}= \\pmb{A}^{-1} \\cdot \\pmb{b}\n",
    "\\end{equation*}\n",
    "\n",
    "But, how do we find $\\pmb{A}^{-1}$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.22222222  0.55555556]\n",
      " [-0.11111111  0.22222222]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "a = np.array([[2., -5.], [1., 2.]])\n",
    "ainv = inv(a)\n",
    "print(ainv)\n",
    "x=np.dot(ainv,b=np.array([3,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus,\n",
    "\\begin{equation*}\n",
    "\\pmb{x}= \\left(\n",
    "\\matrix{\n",
    "0.222 & 0.556 \\\\\n",
    "-0.111 & 0.222 } \\right)\n",
    "\\cdot \\left( \n",
    "\\matrix{ 3 \\\\ 2} \\right)= \n",
    "\\left(\\matrix{1.778\\\\ 0.111} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "And we can verify that $\\pmb{A}\\cdot \\pmb{x}= \\pmb{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a,x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Note. Not all the systems of equations have a solution and some have infinite number of solutions.\n",
    "\n",
    "For instance, \n",
    "\\begin{align*}\n",
    "x_1-x_2 +x_3 -x_4 &=1\\\\\n",
    "x_1-x_2-x_3+x_4 &=0\\\\\n",
    "x_1-x_2-2x_3+2x_4&= -\\frac{1}{2}\n",
    "\\end{align*}\n",
    "or in matrix form\n",
    "\\begin{equation*}\n",
    "\\left(\n",
    "\\begin{matrix}\n",
    "1 & -1 & 1 & -1 \\\\\n",
    "1 & -1 & -1 & 1 \\\\\n",
    "1 & -1 & -2 & 2 \n",
    "\\end{matrix} \\right)\n",
    "\\cdot \\pmb{x}= \\left( \n",
    "\\begin{matrix}\n",
    "1 \\\\\n",
    "0\\\\\n",
    "-\\frac{1}{2}\n",
    "\\end{matrix}\n",
    "\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "In this case any expression that satisfies $x_2=x_1 -1/2$ and $x_3=x_4+1/2$ for arbitrary values of $x_1$ and $x_4$ will satisfy the system.\n",
    "So, what is the problem?\n",
    "\n",
    "First, there are more $x$'s than equations. But not only that, we have to make sure that the equations that we add make rows *linearly dependent*. This means that they cannot be additions(substractions) or constant multiplication of the previous rows. \n",
    "\n",
    "The **rank** of a matrix $A$ (written as $rank(A)$) is the number of *linearly independent* rows (or columns). \n",
    "\n",
    "For example, let's examine the first and second columns of the matrix $A$\n",
    "\\begin{equation*}\n",
    "A=\\left(\n",
    "\\begin{matrix}\n",
    "1 & -1 & 1 & -1 \\\\\n",
    "1 & -1 & -1 & 1 \\\\\n",
    "1 & -1 & -2 & 2 \n",
    "\\end{matrix} \\right)\n",
    "\\end{equation*}\n",
    "We observe that $(-1,-1,-1)^t= (-1)*(1,1,1)^t$, so the second row can be obtained by multiplying $-1$ to the first row. Thus, the first and second columns are *linearly dependent*. Similarly, the third and four row satisfy $(1,-1,-2)^t= (-1)*(-1,1,2)^t$, thus these rows are linearly dependent. Finally, there is no real number $\\alpha$ that satisfies $(1,1,1)^t= \\alpha (1,-1,-2)^t$, thus both $(1,1,1)^t$ and $(1,-1,-2)$ are *linearly independent*. Therefore, we conclude that $rank(A)$=2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Determinants\n",
    "\n",
    "Let's check quickly the what is the definition of the **determinant** of a matrix $A$ of type $n\\times n$.\n",
    "\n",
    "Case $n=2$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{det}\\left[ \n",
    "\\begin{matrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22} \n",
    "\\end{matrix}\n",
    "\\right]= (a_{11}* a_{22}) - (a_{21}* a_{12})\n",
    "\\end{equation*}\n",
    "\n",
    "Case n=3\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{det}\n",
    "\\begin{pmatrix}\n",
    "a_{11} & a_{12} & a_{13} \\\\\n",
    "a_{21} & a_{22} & a_{23} \\\\\n",
    "a_{31} & a_{32} & a_{33}\n",
    "\\end{pmatrix}\n",
    "= a_{11} \\cdot \\mathrm{det} \n",
    "\\begin{pmatrix}\n",
    "a_{22} & a_{23} \\\\\n",
    "a_{32} & a_{33}\n",
    "\\end{pmatrix} \n",
    "- a_{12}\\cdot \\mathrm{det}\n",
    "\\begin{pmatrix}\n",
    "a_{21} & a_{13} \\\\\n",
    "a_{31} & a_{33} \n",
    "\\end{pmatrix} \n",
    "+a_{13} \\cdot \\mathrm{det}\n",
    "\\begin{pmatrix}\n",
    "a_{21} & a_{22} \\\\\n",
    "a_{31} & a_{32} \n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Matrices of type $n\\times n$ that have determinant different from zero are called **non-singular**. These matrices will have an **inverse matrix** that satisfies\n",
    "\n",
    "\\begin{equation*}\n",
    "A \\cdot A^{-1} = A^{-1} \\cdot A = I_n,\n",
    "\\end{equation*}\n",
    "where $I_n$ is the identity matrix (a matrix of type $n\\times n$ that has 1's in the diagonal and zeros elsewhere)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let $A$ be the $3\\times3$ matrix\n",
    "\n",
    "\\begin{equation}\n",
    "A=\n",
    "\\begin{pmatrix}\n",
    "4 & 3 & 2 \\\\\n",
    "3& 5 & 2 \\\\\n",
    "2 & 2 &1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "What is the inverse matrix $A^{-1}$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.00000000e+00, -1.00000000e+00,  4.00000000e+00],\n",
       "       [-1.00000000e+00, -3.22973971e-16,  2.00000000e+00],\n",
       "       [ 4.00000000e+00,  2.00000000e+00, -1.10000000e+01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A=np.array([[4.,3.,2.],[3.,5.,2.],[2.,2.,1.]])\n",
    "from numpy.linalg import inv\n",
    "\n",
    "inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution of Equations and Determinants\n",
    "\n",
    "There is a simple way to find out wheter the system with the same number of equation than unknowns has a solution and if so this one would be unique. We apply the *determinant* function to the matrix of the system. If this determinant is different from zero, we will have a *unique solution*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "We will talk about special types of matrices that are of type $n\\times n$\n",
    "+ A matrix $A$ is called *symmetric* if $A=A^t$.\n",
    "+ A matrix $A$ is called *orthogonal* if $A^t A=I_n$.\n",
    "\n",
    "A number $\\lambda$ is called *eigenvalue* of the matrix $A$ if there exist a non-zero vector $\\pmb{x} \\in \\mathbb{R}^n$ such that\n",
    "\\begin{equation}\n",
    "A \\pmb{x} = \\lambda \\pmb{x} \n",
    "\\end{equation}\n",
    "The vector $\\pmb{x}$ is called *eigenvector* with corresponding *eigenvalue* $\\lambda$.\n",
    "> Note that $\\lambda$ could be a complex number\n",
    "\n",
    "**Theorem**. \n",
    "Let $A$ be a symmetric matrix. Then $A$ has $n$ real eigenvalues, not necessarily distinct, $\\lambda_1,\\ldots, \\lambda_n$ and $n$ eigenvectors $\\pmb{x}_1,\\ldots,\\pmb{x}_n$ which form an orthonormal basis in $\\mathbb{R}^n$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo inverse\n",
    "\n",
    "Let's consider a more general linear system\n",
    "\\begin{equation*}\n",
    "A \\pmb{x}= \\pmb{b}\n",
    "\\end{equation*}\n",
    "where the matrix $A$ is of type $m\\times n$ with $m>n$, $\\pmb{x}$ is an $n$-dimensional vector and $\\pmb{b}$ is an $m$-dimensional vector. \n",
    "\n",
    "> Note that this case includes several datasets in biomedical informatics. For instance in RNA-seq data where the number of genes (rows) and columns are the number of samples which are normally less. \n",
    "\n",
    "Since $A$ is not an square matrix, we cannot define $A^{-1}$. However, there is a *good chance* that the matrix $A^t A$ is invertible. We then define the matrix pseudo inverse of $A$ as\n",
    "\\begin{equation*}\n",
    "A^+ = (A^t A)^{-1} A^t\n",
    "\\end{equation*}\n",
    "If $\\pmb{x}^*$ satisfies $A \\pmb{x}^*=b$, then\n",
    "\\begin{equation*}\n",
    "A\\pmb{x}^*=b \\iff A^t A \\pmb{x}^* = A^t b \\iff \\pmb{x}^*=(A^t A)^{-1}A^t b\\iff \\pmb{x}^*=A^+ b\n",
    "\\end{equation*}\n",
    "We say then that $\\pmb{x}^*$ represents the pseudo inverse solution of $Ax=b$. \n",
    "> Note that the pseudo inverse representation is only valid for the case $n< m$.\n",
    "\n",
    "### Geometric Interpretation of the pseudo inverse solution\n",
    "\n",
    "Let's consider the linear mapping $F\\colon \\mathbb{R}^n \\to \\mathbb{R}^m$ defined as $F(\\pmb{y})= A \\pmb{y}$ with $n<m$. \n",
    "Further, let's consider that there is a vector $\\pmb{y}^*\\in \\mathbb{R}^n$ that satisfies\n",
    "\\begin{equation*}\n",
    "\\pmb{y}^*= \\text{argmin}_{\\pmb{y}\\in \\mathbb{R}^n} \\| A \\pmb{y} -b \\|_2\n",
    "\\end{equation*}\n",
    "Thus, $A \\pmb{y}^*= \\overline{b}$. Moreover, it can be shown that $\\overline{b}=A A^+ b$ and thus $\\pmb{y}^* = A^+ b$. Therefore $\\pmb{y}^*$ represents the pseudo inverse solution of $A y=b$. \n",
    "\n",
    "<img src=\"../../Figures/fig_pseudo_inverse.png\" width=600>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices as transformations\n",
    "\n",
    "Among the different functions say from $f \\colon \\mathbb{R}^2\\to \\mathbb{R}^2$, the so-called **linear transformation** play a very important role in mathematics. Such transformations can be represented by a matrix (defined by the transformation itself). Let's suppose we have a vector $\\pmb{x}=(x_1,x_2)^t$ (note that we write $\\pmb{x}$ as a column vector) and a matrix $\\pmb{A}$ (of type $2\\times2$ in this case), so we can define a linear transformation \n",
    "\\begin{equation}\n",
    "T_A(\\pmb{x}) = \\pmb{A} \\cdot \\pmb{x}\n",
    "\\end{equation}\n",
    "What does the transformation $T_A$ do to any vector $\\pmb{x}$?\n",
    "\n",
    "Here are some possibilitites\n",
    "\n",
    "+ It keeps the same vector (trivial transformation)\n",
    "\n",
    "\\begin{equation}\n",
    "\\pmb{I}_2 \\cdot \\pmb{x}= \\pmb{x}\n",
    "\\end{equation}\n",
    "\n",
    "+ Expands the vector in a given direction\n",
    "$\\pmb{x}=\\pmatrix{x_1\\\\x_2}$\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "2 & 0\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix} \\cdot \\pmb{x} = (2x_1,x_2)^t= \\pmatrix{2x_1\\\\x_2}\n",
    "\\end{equation}\n",
    "\n",
    "+ Rotates the vector $180^\\circ$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "-1 & 0\\\\\n",
    "0 & -1\n",
    "\\end{pmatrix} \\cdot \\pmb{x} =\\pmatrix{-x_1\\\\ -x_2}\n",
    "\\end{equation}\n",
    "\n",
    "+ Rotates the vector with a specific angle $\\theta$\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "\\cos(\\theta) & \\sin (\\theta)\\\\\n",
    "-\\sin (\\theta) & \\cos(\\theta)\n",
    "\\end{pmatrix} \\cdot \\pmb{x} =\\pmatrix{x_1 \\cos(\\theta) +x_2 \\sin(\\theta)\\\\ -x_1\\sin(\\theta)+ x_2 \\cos(\\theta)\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "+ It can project the vector to each coordinate axis\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "0 &0 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{pmatrix} \\cdot \\pmb{x}= \\pmatrix{0 \\\\ x_2}\n",
    "\\end{equation}\n",
    "\n",
    "## Rigid transformations\n",
    "\n",
    "From basic algebra we know that the equation $y=mx +b$ represents a line in the plane with slope $m$ and $y$-intercept $b$. However, a similar expression such as\n",
    "\\begin{equation}\n",
    "\\pmb{A} \\cdot \\pmb{x} + \\pmb{b}\n",
    "\\end{equation}\n",
    "does not represent a linear transformation! This transformation is a translation of every vector by the vector $\\pmb{b}$\n",
    "\n",
    "For instance, the rigid-transformation $T$ defined as\n",
    "\\begin{equation}\n",
    "T(\\pmb{x})= \\pmatrix{\n",
    "3 & 0 \\\\\n",
    "2 & 1 } \\cdot \\pmb{x}+ \\pmatrix{1\\\\1}\n",
    "\\end{equation}\n",
    "\n",
    "For $e_1=(1,0)$ and $e_2=(0,1)$\n",
    "\n",
    "<img src=\"../../Figures/fig_rigid_s.png\" width=500>\n",
    "\n",
    "## Composition of transformations\n",
    "\n",
    "In mathematics, a composition of functions refers to the operation of putting functions in tandem. That is, applying a function to a point (vector) and taking the output to put it as input of another function and so on. \n",
    "\n",
    "Let $T_A \\colon \\mathbb{R}^m\\to \\mathbb{R}^k$ and $T_B\\colon \\mathbb{R}^n \\to \\mathbb{R}^m$ two linear transformations, then the composition $T_A$ with $T_B$ is the (linear) transformation $T_{A B}\\colon \\mathbb{R}^n \\to \\mathbb{R}^k$ defined as\n",
    "\n",
    "\\begin{equation}\n",
    "T_{A B} ( \\pmb{x})= T_A\\circ T_B(\\pmb{x}))= T_A(T_B(\\pmb{x}))= \\pmb{A} \\cdot \\pmb{B} \\cdot \\pmb{x}\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Geometric properties\n",
    "\n",
    "The **dot** product on arrays (or vectors) have a nice geometric interpretation\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{x} \\cdot \\pmb{y} = \\| \\pmb{x}\\| \\| \\pmb{y} \\| \\cos( \\theta)\n",
    "\\end{equation*}\n",
    "where $\\theta$ is the angle between the two vectors. This property is very handy when we want to find **perpendicular vectors**, in which case $\\theta =90^\\circ$ and the dot product will be zero!)\n",
    "\n",
    "\n",
    "There is another operation between vectors in higher dimensions, the so-called **cross-product**. Whereas the previous product results in a number, the cross-product represents another vector.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\pmb{x} \\times \\pmb{y}= \\textrm{det } \n",
    "\\pmatrix{ \\pmb{i} & \\pmb{j} & \\pmb{k} \\\\\n",
    "x_1 & x_2 & x_3\\\\\n",
    "y_1 & y_2 & y_3 }\n",
    "= \\pmb{i}(x_2y_3 - y_2 x_3)+ \\pmb{j}(x_3y_1-x_1 y_3)+ \\pmb{k}(x_1 y_2 -x_2y_1)\n",
    "\\end{equation*}\n",
    "where $\\pmb{i}= (1,0,0)$, $\\pmb{j}=(0,1,0)$ and $\\pmb{k}=(0,0,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, -6, -3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = [1, 2, 0]\n",
    "y = [4, 5, 6]\n",
    "np.cross(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus refresher: Derivatives\n",
    "\n",
    "### Case $n=1$\n",
    "We will begin with elementary calculus. Let $f\\colon \\mathbb{R} \\to \\mathbb{R}$ be a smooth (differentiable) function at the point $x_0$. \n",
    "\n",
    "What does the derivative of $f$ at $x_0$ represents? In theory, it represents the slope of the tangent line of $f(x)$ at $x_0$ (for points *close* to $x_0$). \n",
    "\n",
    "\\begin{equation}\n",
    "f'(x_0) \\approx \\frac{f(x)-f(x_0)}{x-x_0} \n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"../../Figures/fig_tangent-crop.gif\" width=750>\n",
    "\n",
    "We can isolate $f(x)$ from the previous expressin and conclude that the derivate provide a linear approximation of a function a the given point for points *close* to $x_0$. More specifically \n",
    "\n",
    "\\begin{equation}\n",
    "f(x) \\approx f(x_0) + f'(x_0) (x-x_0)\n",
    "\\end{equation}\n",
    "for points close to $x_0$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case $n\\ge2$\n",
    "\n",
    "In higher dimensions, we consider the concept of **partial derivative**. For instance, if $f\\colon \\mathbb{R}^2 \\to \\mathbb{R}$ given by $f(x,y)= x^2+2y+1$. Then, we repeat the idea of approaching to the point $(x_0,y_0)$ for the $x$-coordinate while keeping the second coordinate fixed at $y_0$. \n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{\\partial f (x_0,y_0)}{\\partial x}= \\lim_{x\\to x_0}\\frac{ f(x,y_0)-f(x_0,y_0)}{x-x_0} \n",
    "\\end{equation*}\n",
    "Similarly for $\\frac{\\partial f (x_0,y_0)}{\\partial y}$. \n",
    "\n",
    "The geometric interpretation is that the partial derivatives define a tangent plane at $(x_0,y_0)$ for the function $f$ defined by the directions of those derivatives evaluated at $(x_0,y_0)$.\n",
    "\n",
    "**Example**. For our function $f$ defined above we have $\\frac{\\partial f (x_0,y_0)}{\\partial x}=2x_0$ and $\\frac{\\partial f (x_0,y_0)}{\\partial y}=2$.\n",
    "\n",
    "\n",
    "Let us first recall the concept of **level surfaces** of a function. If $f\\colon \\mathbb{R}^2 \\to \\mathbb{R}$, we can represent the graph of $f$ in a 3-d plot as a surface. If the function reaches a level $c$ (i.e., there is $\\pmb{x}_0$ such that $f(\\pmb{x}_0)=c$), then the set of all points in the plane that have the same value $c$ are called the level surface at level $c$.\n",
    "\n",
    "\n",
    "\n",
    "Another example is the temperature map\n",
    "\n",
    "<img src=\"../../Figures/nooa_fig.png\" width=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher order derivatives\n",
    "\n",
    "Depending on the function definition, we can continue with the process of taking partial derivatives. For instance if $f(x,y)$ admits to take derivatives consecutively (a condition normally refered to as a function being of class $C^2(\\mathbb{R}^2)$, then we can define\n",
    "the *second* derivatives of $f(x,y)$ as follows\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial y}\\left( \\frac{\\partial f(x,y)}{\\partial x} \\right)&= \n",
    "\\frac{\\partial^2 f(x,y)}{\\partial y \\partial x}\\\\\n",
    "\\frac{\\partial}{\\partial y}\\left( \\frac{\\partial f(x,y)}{\\partial y} \\right)&= \n",
    "\\frac{\\partial^2 f(x,y)}{\\partial y^2}\\\\\n",
    "\\frac{\\partial}{\\partial x}\\left( \\frac{\\partial f(x,y)}{\\partial x} \\right)&= \n",
    "\\frac{\\partial^2 f(x,y)}{ \\partial x^2}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Further, in this scenario it follows that\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial^2 f(x,y)}{\\partial y \\partial x}=\\frac{\\partial^2 f(x,y)}{\\partial x \\partial y}\n",
    "\\end{equation}\n",
    "\n",
    "There is an important matrix related to this process and it is called the **Hessian Matrix** of the function $f(x,y)$ and it is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "H(x,y)= \\pmatrix{\n",
    "\\frac{\\partial^2 f(x,y)}{\\partial x^2} & \\frac{\\partial^2 f(x,y)}{\\partial x \\partial y} \\\\\n",
    "\\frac{\\partial^2 f(x,y)}{\\partial y \\partial x} & \\frac{\\partial^2 f(x,y)}{\\partial y^2}}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Maximum and Minimum of functions\n",
    "\n",
    "One of the major tasks in Machine Learning is to find *optimal* values, which in mathematical terms refers to identify points in which the function attains its maximum or minimum value either globally or locally. \n",
    "\n",
    "> Let's assume that the functions $f(x_1,\\ldots, x_n)$ defined in this section are of class $C^\\infty$ where consecutive derivatives of any order exist and are continuous either on the real line or in higher dimension $\\mathbb{R}^n$. \n",
    "\n",
    "### Case $n=1$.\n",
    "\n",
    "The function $f(x)$ can have a maximum or minimum only at the points where its derivative is equal to zero or does not exist. \n",
    "\n",
    "Thus, if the point $x=x_0$ is a maximum or minimum then $f'(x_0)=0$. \n",
    "\n",
    "Further, \n",
    "\n",
    "+ if $f''(x_0)<0$ then the point $x_0$ is a (local) maximum.\n",
    "+ if $f''(x_0)>0$ then the point $x_0$ is a (local) minimum.\n",
    "+ if $f''(x_0)=0$, and $f'''(x_0) \\ne 0$, then $x_0$ is an inflection point.\n",
    "\n",
    "### Case $n=2$.\n",
    "\n",
    "The function $f(x,y)$ has a maximum or miminum at the point $(x_0,y_0)$ when\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(x_0,y_0)}{\\partial x}=0 \\quad \\text{and} \\quad \\frac{\\partial f(x_0,y_0)}{\\partial y}=0\n",
    "\\end{equation}\n",
    "\n",
    "Let's call $\\Delta(x_0,y_0)= \\det H(x_0,y_0)$. \n",
    "\n",
    "+ if $\\Delta >0$ and $\\frac{\\partial^2 f(x_0,y_0)}{\\partial x^2}>0$, then the point $(x_0,y_0)$ is a minimum.\n",
    "+ if $\\Delta >0$ and $\\frac{\\partial^2 f(x_0,y_0)}{\\partial x^2}<0$, then the point $(x_0,y_0)$ is a maximum.\n",
    "+ if $\\Delta <0$ the function $f(x,y)$ does not have an extreme point.\n",
    "+ if $\\Delta=0$ we need further investigation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor's Theorem\n",
    "\n",
    "Taylor's theorem provides a mechanism to calculate approximate values of functions.\n",
    "\n",
    "### Case $n=1$.\n",
    "\n",
    "If $\\theta \\in (0,1)$, and \n",
    "\\begin{equation}\n",
    "f(x_0+h)= f(x_0)+ \\frac{h}{1!}f'(x_0)+ \\frac{h^2}{2!}f''(x_0)+ \\cdots+ \\frac{h^k}{k!}f^{(k)}(x_0+h\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "Thus, we can say that for *very small* values of $h$\n",
    "\\begin{equation}\n",
    "f(x_0+h) \\sim f(x_0) + h f'(x_0)\n",
    "\\end{equation}\n",
    "\n",
    "### Case $n=2$.\n",
    "For relatively small $h$ and $k$ values, we have \n",
    "\n",
    "\\begin{align}\n",
    "f(x_0+h,y_0+k)=& f(x_0,y_0) + \\frac{\\partial f(x_0,y_0)}{\\partial x}h + \\frac{\\partial f(x_0,y_0)}{\\partial y}k  \\\\\n",
    "&+ \\frac{1}{2!}\\left(\n",
    "\\frac{\\partial^2 f(x_0,y_0)}{\\partial x^2}h^2 + \\frac{\\partial^2 f(x_0,y_0)}{\\partial y^2}k^2 + 2 \\frac{\\partial^2 f(x_0,y_0)}{\\partial x \\partial y}hk   \n",
    "\\right)\\\\\n",
    "&+ \\text{Higher Order Terms}\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The gradient\n",
    "\n",
    "Let's suppose now that $f\\colon \\mathbb{R}^n \\to \\mathbb{R}$ a smooth (differentiable) function at the point (vector) $\\pmb{x}_0=(x_1^0,x_2^0,\\ldots,x_n^0)$. The **gradient** of the function $f$ (in cartesian coordinates) is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{grad }f(\\pmb{x}_0)= \\nabla f(\\pmb{x}_0)= \\left( \\frac{\\partial f (\\pmb{x_0})}{\\partial x_1},\\frac{\\partial f (\\pmb{x_0})}{\\partial x_2},\\ldots,\\frac{\\partial f (\\pmb{x_0})}{\\partial x_n}  \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Recall that the partial derivatives represent derivatives of the function $f$ with respect to a given variable while considering the rest of them constant. \n",
    "\n",
    "How can we interpret the gradient?\n",
    "\n",
    "+ It is a vector whose direction is always perpendicular to the direction of the level surface passing through the point.\n",
    "\n",
    "+ The gradient vector always points to the direction in which the function $f$ is increasing.\n",
    "\n",
    "\n",
    "### Example.\n",
    "\n",
    "Consider $f(x_1,x_2)= 20 \\cos(36*(x_1^2+x_2^2))+30$. The surface levels are depicted in the following animation.\n",
    "\n",
    "<img src=\"../../Figures/fig_level-crop.gif\" width=550>\n",
    "\n",
    "Let's suppose we have a value $c=40$. The level-surface $L_c$ is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "L_c= \\{ (x_1,x_2): f(x_1,x_2)=40\\}= \\{(x_1,x_2): x_1^2+x_2^2= \\frac\\pi{108} \\}\n",
    "\\end{equation}\n",
    "Thus, $L_c$ represents a circle of radius $\\sqrt{\\pi/108}$ centered at the origin. The gradient would be\n",
    "\\begin{equation}\n",
    "\\nabla f(x_1,x_2)= \\left( \n",
    "-1440 \\sin(36*(x_1^2+x_2^2))x_1 , -1440 \\sin(36*(x_1^2+x_2^2))x_2\n",
    "\\right)\n",
    "\\end{equation}\n",
    "For a point $(x_1^0,x_2^0) \\in L_c$, \n",
    "\\begin{equation}\n",
    "\\nabla f(x_1^0,x_2^0)= -720\\sqrt{3} (x_1^0,x_2^0)\n",
    "\\end{equation}\n",
    "Thus, the gradient points towards the origin for every point in the level surface $L_c$.\n",
    "\n",
    "<img src=\"../../Figures/fig_gradient.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An important expansion\n",
    "\n",
    "Let's suppose we have a vector $\\theta \\in \\mathbb{R}^n$, and let's represent a *small* change of this vector by $\\Delta \\theta$ (note $\\Delta \\theta$ is also a vector in $\\mathbb{R}^n$. \n",
    "\n",
    "Let's assume further that we have a function $C \\colon \\mathbb{R}^n \\to \\mathbb{R}$ that is of class $C^\\infty$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "C(\\theta + \\Delta\\theta)= C(\\theta)+ \\Delta \\theta^T \\nabla C(\\theta) + \\frac{1}{2} \\Delta \\theta^T H(\\theta) \\Delta \\theta + \\text{Higher Order Terms}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Let $I_1,\\ldots, I_n \\subset \\mathbb{N}$. be $n$ subsets of the natural numbers. \n",
    "\\begin{equation*}\n",
    "I_1 \\times \\cdots \\times I_n = \\{ (i_1,\\ldots, i_n): i_k \\in I_k, 1\\le k\\le n\\}\n",
    "\\end{equation*}\n",
    "\n",
    "A set of object indexed over the multi-index $(i_1,i_2,\\ldots,i_n)$ is called a **tensor** or oder $n$ and typically denoted by $T_{i_1,\\ldots,i_n}$.\n",
    "\n",
    "+ A vector $\\pmb{x}\\in \\mathbb{R}^d$, $\\pmb{x}=(x_1,\\ldots, x_d)$ is a tensor of order 1 and type $d$. \n",
    "+ A matrix $A=(a_{ij}) \\in \\mathbb{R}^{d\\times r}$ is a tensor of order 2 and type $d\\times r$. \n",
    "+ A tensor of order 3, $t \\in \\mathbb{R}^{d\\times r \\times s}$ can be viewed as a vector of length $s$ of matrices of type $d\\times r$.\n",
    "\n",
    "**Example** A color image is represented as a tensor of order 3 of type $n\\times m \\times 3$ where $n$ is the number of pixels lines, $m$ is the number of columns , and 3 stands for the number of color channels in the RGB format. \n",
    "\n",
    "<img src=\"../../Figures/fig_tensor.png\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Numbers\n",
    "\n",
    "Can a deterministic machine produce random numbers? \n",
    "In principle, any program in a deterministic machine will produce an output that is entirely predictable, thus not random. \n",
    "\n",
    "Our computers produce *pseudo*-random numbers. At the minimum, we expect that a random number generator produces different sequences which are statistically uncorrelated to each other. \n",
    "\n",
    "When we invoke a random number generator, we have access to a large sequence of numbers. The term **seed** refers to the initialization point of such sequence. Consecutive calls to the generator will give us the desired sequence or \"random\" numbers. \n",
    "\n",
    "The basic module *random* generates uniform random numbers, but it is possible to use *numpy*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniformly Distributed Random numbers between (0,1) \n",
      "0.5316\n",
      "0.4426\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(2022)\n",
    "print(\"Uniformly Distributed Random numbers between (0,1) \")\n",
    "print(\"%.4f\"%random.uniform(0,1))\n",
    "print(\"%.4f\"%random.uniform(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniformly Distributed Random numbers between (0,1) \n",
      "0.0094\n",
      "0.4991\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2022)\n",
    "print(\"Uniformly Distributed Random numbers between (0,1) \")\n",
    "print(\"%.4f\"%np.random.uniform(0,1))\n",
    "print(\"%.4f\"%np.random.uniform(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify emprically, how accurate are our random number generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean = 0.00000 and Emprical mean= 0.01707\n",
      "Standard Deviation = 1.00000 and Empirical Stdev = 0.99020\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "my_mean=0; my_stdev=1.0; n=5000\n",
    "u=np.random.normal(my_mean,my_stdev,n)\n",
    "m=sum(u)/n; s =math.sqrt(sum((u-my_mean)**2)/(n-1))\n",
    "print(\"Mean = %.5f and Emprical mean= %.5f\"%(my_mean,m))\n",
    "print(\"Standard Deviation = %.5f and Empirical Stdev = %.5f\"%(my_stdev,s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "citation-manager": {
     "citations": {
      "zv3ro": [
       {
        "id": "11398979/CPHH7UIN",
        "source": "zotero"
       }
      ]
     }
    },
    "tags": []
   },
   "source": [
    "## Sources and references\n",
    "\n",
    "Some figures and material were adapted from <cite id=\"zv3ro\">(Calin, 2020)</cite>. Some figures needed tricks from tikz, and other online hacks on LaTeX.\n",
    "\n",
    "\n",
    "<!-- BIBLIOGRAPHY START -->\n",
    "<div class=\"csl-bib-body\">\n",
    "  <div class=\"csl-entry\">Calin, O. (2020). <i>Deep Learning Architectures. A mathematical approach</i>. Springer.</div>\n",
    "\n",
    "</div>\n",
    "<!-- BIBLIOGRAPHY END -->"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "citation-manager": {
   "items": {
    "zotero": {
     "11398979/CPHH7UIN": {
      "ISBN": "978-3-030-36723-7",
      "author": [
       {
        "family": "Calin",
        "given": "Ovidiu"
       }
      ],
      "collection-title": "Springer Series in the Data Sciences",
      "id": "11398979/CPHH7UIN",
      "issued": {
       "date-parts": [
        [
         2020
        ]
       ]
      },
      "publisher": "Springer",
      "title": "Deep Learning Architectures. A mathematical approach",
      "type": "book"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
