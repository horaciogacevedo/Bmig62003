\documentclass{beamer}
\DeclareFontShape{OT1}{cmss}{b}{n}{<->ssub * cmss/bx/n}{} 
\usetheme{default}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{amstex}
\usepackage{mathbbol}
\usepackage{xcolor} % before tikz or tkz-euclide if necessary
\usepackage{tkz-euclide} % no need to load TikZ
\usepackage{multirow}
\usepackage{lmodern}
\usepackage{bm}

\usepackage[
backend=biber,
style=authoryear-icomp,
sortlocale=de_DE,
natbib=true,
url=false, 
doi=true,
eprint=false
]{biblatex}
\addbibresource{../../Bibliography/main_ML.bib}


\titlegraphic{\includegraphics[width=2cm]{../../Figures/UAMS_RGB.png}
}

\title{Statistical Machine Learning\\ 
Tree-Based Methods}
\author{Horacio G\'omez-Acevedo\\ Department of Biomedical Informatics\\
University of Arkansas for Medical Sciences}
\begin{document}
	\begin{frame}[plain]
		\maketitle
	\end{frame}
	
	
	\begin{frame}{Tree structures}
		
		A typical tree is depicted with the root being the top node, and growing down. Decisions are being made at each node until a terminal node or {\it leaf} is reached. Each non-terminal node contains a question on which a split is based. Each leaf contains the label (classification) or the predicted mean value (regression).
		 
		
		
\end{frame}

\begin{frame}{Regression Trees}

We will use the {\bf Hitters} data set as an example for  regression.  After running the code we obtain the following figure

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{../../Figures/fig_hitters.png}
\end{figure}
	
	
\end{frame}

\begin{frame}{Regression Trees (cont)}
	How to read this information?
	The first feature for this set is {\it years}, from which the total samples are split into two depending on whether their time spent in the baseball league has been less than 4.5 years. The case on the left (True) is already given a value of $5.107$, which means that the average salary is $\exp(5.107)\approx 165.17$ thousand dollars a year. Players with more than 4.5 years will be further divided into the variable ${\it hits} $, and someone above 117.5 will earn on average $\exp(6.74)=845.56$ thousand dollars!
\end{frame}

\begin{frame}{Regression Trees (cont)}
	We can see that the graphical representation of a tree is as follows
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../../Figures/fig_hitters_graph.png}
	\end{figure}
The regions $R_i$ are the representing the leaves of the tree.  	
	
\end{frame}

\begin{frame}{Prediction in Regression Trees}
	There are two main steps involved in building a tree
	
	\begin{itemize}
		\item Divide the predictor space $X_1 \times X_2 \times \cdots \times X_p$ into $J$ distinct and non-overlapping regions $R_1,\ldots, R_J$.
		\item For every observation that falls into the region $R_j$, we make the same prediction, which is the mean of the response values for the training observations in $R_j$.
	\end{itemize}
\end{frame}

\begin{frame}{2D Example}
	Suppose we have the outcome $Z$ depending on two predictors $X$ and $Y$ defined in certain rectangular area $A\subset X \times Y$ of the plane. One feasible partition of $A$ is depicted below
		\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../../Figures/fig_tree_region.png}
	\end{figure}
	
\end{frame}

\begin{frame}{2D Example (cont)}
The previous partition can be described as a tree structure
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{../../Figures/fig_tree_trans.png}
\end{figure}

The corresponding prediction for $\widehat{Z}(x,y)$ based on given partition of $A$ is defined as 
\begin{equation*}
	\widehat{Z}(x,y)= \textrm{Average} \{ Z(x_r,y_r)\colon (x_r ,y_r)\in R_j)\}
\end{equation*}
where $(x,y) \in R_j$.

\end{frame}

\begin{frame}{Dividing the Predictor Space}
	Ideally, one must find regions $R_1, \ldots, R_J$ (also called {\bf boxes}) that minimize the $RSS$ given by 
	\begin{equation*}
		\sum_{j=1}^J \sum_{i\in R_j} (y_i - \hat{y}_{R_j})^2
	\end{equation*}
where $\hat{y}_{R_j}$ is the mean response for the training observations within the $j$th box. Since it is not computationally feasible to test every single box combination, we use a {\it greedy} approach that is known as {\it recursive binary splitting}. This process is top down since we start with the whole region and after a number of successive splits into two branches further down on the tree. At each step we select the {\it best} split at the given time without looking at other alternatives further down. 
\end{frame}

\begin{frame}{Recursive Binary Splitting}
	We begin by selecting a predictor $X_j$ and the cut point $s$ such a that splitting the predictor space into the regions $\{ X| X_j <s \}$ and $\{ X | X_j \ge s\}$ leads to the creates possible reduction in RSS. Thus, for any $j$ and $s$, we define a pair of half-planes
	\begin{equation}
		R_1(j,s)= \{ X | X_j <s\} \quad \textrm{and } R_2(j,s)= \{X| X_j\ge s\}
		\label{eq:rbs1}
	\end{equation}
and we look for the value $j$ and $s$ that minimize the equation
\begin{equation}
	\sum_{i: x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 + 	\sum_{i: x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2 ,
	\label{eq:rbs2}
\end{equation}
We repeat the process looking for the best predictor and best cut point in order to split the data further so as to minimize the RSS within each of the resulting regions but this time we we don't split the entire space but each of the two regions found. This process continues until a certain criteria is reach (e.g., until no region contains fewer than a certain number of elements within). 


\end{frame}

\begin{frame}{Recursive Binary Splitting (cont)}

Once the regions are found, the predicted response for a given test observation is the mean of the training observations in the region to which that test observation belongs.

The recursive binary splitting may produce good predictions on the training set, but it follows to close the data and more likely produce overfit. Thus, a smaller tree with fewer splits might lead to lower variance and better interpretation at the cost of bias.
\end{frame}

\begin{frame}{Tree Pruning}
	
	
	One way to address the overfiting problem is to grow a very large tree $T_0$ , and then prune it back to obtain a sub-tree. Ideally, we select a sub-tree that leads to the lowest test error rate using say cross-validation. 
	
	Once again we faced the  problem that the number of possible sub-trees is computationally intensive.
	
	A process known as {\it weakest link pruning} considers a sequence of trees indexed by a non-negative tuning parameter $\alpha$. For each value of $\alpha$ there corresponds a sub-tree $T \subset T_0$ such that the following quantity is as small as possible
	
	\begin{equation}
		\sum_{m=1}^{|T|} \sum_{i\in R_m} (y_i -\hat{y}_{R_m})^2 + \alpha |T|,
	\end{equation}
	 where $|T|$ denotes the number of leaves of the tree $T$.
\end{frame}


\begin{frame}{Regression Tree Algorithm}
	\begin{enumerate}
		\item Use recursive binary splitting to grow a large tree on the training data. Use a stopping rule such as the minimum number of observations on each leaf.
		\item Apply the weakest link pruning  to the large tree in order to obtain a sequence of best sub-trees as a function of a parameter $\alpha$. 
		\item Use $K$-fold cross-validation to choose $\alpha$. More precisely, divide the training observations into $K$ folds, and for $k \in \{1,\ldots, K\}$ do:
		\begin{enumerate}
			\item Repeat steps 1 and 2 on all but the $k$th fold of the training data.
			\item Evaluate the mean squared prediction error on the data in the left-out $k$th fold, as a function of $\alpha$
		\end{enumerate} 
		Average the results for each value of $\alpha$, and pick $\alpha$ that minimize the average error.
		\item Return the sub-tree from step 2 that corresponds to the chosen value of $\alpha$.
	
	\end{enumerate}
\end{frame}

\begin{frame}{Example. }
	The unpruned tree that results from greedy splitting on the training data from the {\bf Hitters} data set is depicted below

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{../../Figures/fig_hitters_full.png}
\end{figure}
\end{frame}

\begin{frame}{Example (cont)}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.4]{../../Figures/fig_hitters_mse.png}
	\end{figure}
\end{frame}

\begin{frame}{Classification Trees}
	The idea of the classification trees is very similar to the regression trees. However, in the classification setting we cannot use RSS as an splitting criteria for the binary selection. Instead, we will use the {\bf classification  error rate}, which is the fraction of the training observations in a region that do not belong to the most common class.
	\begin{equation*}
		E= 1- \max_k (\hat{p}_{mk})
	\end{equation*} 
	where $\hat{p}_{mk}$ represent the proportion of training observations in the $mt$h region that are from the $k$th class.	
	

\end{frame}

\begin{frame}{Purity metrics}
	
	A node is {\bf pure} if all training instances belong to the same class. 
	Some metrics to asses the impurity are:
	\begin{itemize}
		\item {\bf Gini index.} Defined by
		
		\begin{equation*}
			G= \sum_{i=1}^K \hat{p}_{mk} (1 - \hat{p}_{mk}),
		\end{equation*}
		is a measure of total variance across the $K$ classes. 
	\item {\bf Cross-entropy. } Defined by
	\begin{equation*}
		D = - \sum_{k=1}^K \hat{p}_{mk}\log \hat{p}_{mk}.
	\end{equation*}	
	Note that both metrics will take on a small value if the $m$th node is pure. Both the Gini index and the cross-entropy are quite similar numerically. 
\end{itemize}
	
\end{frame}

\begin{frame}{Example}
	
	Using the {\bf Heart} data, the classification tree (unpruned) is shown below. 
	\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{../../Figures/fig_class_tree.png}
\end{figure}	
Note the leave {\tt RestECG<1} . The response is Yes so why do we need to partition further? Then answer is that it does not reduce the classification error but  it improves the Gini index and cross-entropy. 
\end{frame}
	
\begin{frame}{Example (cont)}
	Cross-validation error, training and test error 
	
		\begin{figure}[h]
		\centering
		\includegraphics[scale=0.4]{../../Figures/fig_class_tree_error.png}
	\end{figure}
	
\end{frame}

\begin{frame}{CART Algorithm}
	G\'eron's book describes the {\it Classification and Regression Tree}  algorithm. Originally described in the book by Breiman et al. (1984) the idea is similar to (\ref{eq:rbs1}) and (\ref{eq:rbs2}), and the cost function that we want to minimize (for classification)
	
	\begin{equation}
		J(s,t_s) = \frac{m_{left}}{m} G_{left}+ \frac{m_{right}}{m} G_{right},
	\end{equation}
where $G_{left}$ is the measure of impurity for the {\it left} subset. 

The same algorithm has the following cost function for regression 

\begin{equation*}
	J(s,t_s)= \frac{m_{left}}{m} RSS_{left}+ \frac{m_{right}}{m} RSS_{right},
\end{equation*}

\end{frame}

\begin{frame}{CART algorithm pruning}

This process consists in searching for the best pruned sub-tree of the maximal tree.  It should be a good intermediate tree between the two extremes

\begin{itemize}
	\item The maximal tree that has a high variance and low bias, and
	\item The tree containing only the root (stump) that has low variance but high bias.
\end{itemize}

Despite the fact that there are a finite number of sub trees, it is computationally expensive to calculate all. Instead, we looks from a sequence of nested sub-trees $T_1,\ldots, T_K$ all pruned from $T_{max}$, where $T_k$ minimizes a penalized criterion where the penalty is proportional to the number of leaves of the tree. 

\end{frame} 

\begin{frame}{CART algorithm pruning (cont)}
Let's consider the regression setup, $t$ denotes a node, and data is represented as pairs $(x_i,y_i)_{i=1}^n$ of explanatory and response variables, respectively.  
\begin{equation*}
\overline{\text{err}}(T)= \frac{1}{n}\sum_{t \text{ leaf of } T} \sum_{(x_i,y_i) \in t} (y_i- \overline{y}_t)^2
\end{equation*}

and the penalized least squares criterion:
\begin{equation*}
	\text{crit}_\alpha= \overline{\text{err}}(T) + \alpha |T|
\end{equation*}
where $|T|$ represents the number of leaves of the tree $T$. 

In \citep{cartbook} is proven that there is a strictly increasing sequence of parameters $(\alpha_i)_{i=1}^K$ such that $\alpha_1=0$ and an associated sequence $T_1 \succ T_2 \succ \cdots \succ T_K$ of nested trees with $T_1=T_{max}$ and 
\begin{equation*}
	\forall \alpha \in [\alpha_k, \alpha_{k+1}[ , \, T_k= \underset{ T \text{ subtree of }T_{max}  }{\textrm{argmin}}
	 \text{crit}_{\alpha} (T) = \underset{ T \text{ subtree of }T_{max}  }{\textrm{argmin}}
	 \text{crit}_{\alpha_k} (T). 
\end{equation*}
\end{frame}

\begin{frame}{CART algorithm pruning (cont)}
	\begin{itemize}
		\item Input: Maximal tree $T_{max}$.
		\item Initialize:
		\begin{enumerate}
			\item $\alpha_1=0$, $T_1 = \underset{ T \text{ pruned from }T_{max}  }{\textrm{argmin}}
			\overline{\text{err}} (T)$; 
			\item $T \leftarrow T_1$; $k\leftarrow1$. 
		\end{enumerate}
	\item Loop while $|T|>1$
	\begin{enumerate}
		\item Calculate $\alpha_{k+1}= \underset{ t \text{ node of }T  }{\min}
		\frac{\overline{\text{err}} (t)- \overline{\text{err}}(T)}{|T_t| -1}$
		\item Prune all $T_t$ branches of $T$ such that
		\begin{equation*}
			\overline{\text{err}}(T_t)+ \alpha_{k+1}|T_t|= \overline{\text{err}}(t)+ \alpha_{k+1}
		\end{equation*}
	\item $T_k$ will be the pruned tree obtained. 
	\item $T\leftarrow T_k$; $k\leftarrow k+1$.
	\end{enumerate}
	\item Output: 
	\begin{enumerate}
		\item Trees: $T_1 \succ T_2 \succ \cdots \succ T_K=\{t_1\}$
		\item Parameters: $\alpha_1=0, \alpha_2, \ldots, \alpha_K$.
	\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}{CART details}
	The choice of the optimal tree can be made directly by minimized the error obtained by cross-validation or by applying the "1 standard error rule".  This rule aims at selecting in the sequence a more compact tree reaching statistically the same error. Thus, it chooses the most compact tree reaching an error lower than the value of the previous minimum augmented by the estimated standard error of this error. 
	
 	Decision Trees have  more robust libraries in R.  Namely, \textbf{rpart} package is particularly useful.
	
\end{frame}

\begin{frame}{Cross-validation in CART}
	The cross-validation (V-fold cross-validation) in \textbf{rpart} goes as follows:
	Denote the whole data set by ${\cal L}_n$ ($n$ is the number of samples). We obtain the sequences $(T_k)_{k=1}^K$ and $(\alpha_k)_{k =1}^K$ as before.  
	As usual in CV, we divide ${\cal L}_n= E_1 \cup E_2 \cup \cdots \cup E_V$. For each $v \in 1,\ldots, V$, we buidl the sequence of subtrees $(T_k^v)_{k=1}^K$ with ${\cal L}_n - E_v$ as a learning sample. We calculate the validation errors of the sequence as
	\begin{equation*}
		R^{cv}(T_k)= \frac{1}{V} \sum_{v=1}^V \sum_{(x_i,y_i)\in E_v} (y_i - T_k^v(x_i))^2,
	\end{equation*}
	where $T_k^v$ minimizes the penalized criterion $\text{crit}_{\alpha'_k}= \sqrt{\alpha_k \alpha_{k+1}}$. 
	Finally, we chose the model $T_{\hat{k}}$ where $\hat{k}= \text{argmin}_{1\le k \le K}R^{cv}(T_k)$
\end{frame}

\begin{frame}{Normalized Penalty CP parameter}
	For the \textbf{rpart} library the so-called CP (normalized complexity-penalty parameter), the value by default is $cp=0.01$, the tree provided corresponds to the one obtained by slecting the one corresponding to $\alpha=0.01* \overline{\text{err}}(T_n)$
\end{frame}

\begin{frame}{Surrogate Splits CART}
	For any given node $t$, let $s^*$ be the best split of the node into $t_L$ (left node) and $t_R$ (right node), for instance the best univariate split.
	
	Take any variable $x_m$ and let $S_m$ be the set of all splits on $x_m$, and $\overline{S}^m$ the set of splits complementary to $S_m$.  For a given split $s_m\in S_m \cup \overline{S}_m$, let  $p(s^*,s_m)$ the probability that $s_m$ predicts $s^*$ correctly.
	
	A split $\tilde{s}_m   \in S_m \cup \overline{S}_m$ is called a \textbf{surrogate split} on $x_m$ for $s^*$ if 
	\begin{equation*}
		p(s^*,\tilde{s}_m)= \max_{s_m} p(s^*,s_m) \quad s_m \in S_m \cup \overline{S}_m
	\end{equation*}
	The intuitive idea of a surrogate split $\tilde{s}_m$ on $x_m$ is the most accurate split that predicts the action of $s^*$. 
\end{frame}

\begin{frame}{Surrogate Splits CART (cont)}
	Suppose $s^*$ sends the cases in $t$ left with relative probability $p_L$ and right with relative probability $p_R$ ($p_R+p_L=1$).
	
	The \textbf{predictive measure of association} 
	\begin{equation*}
		\lambda(s^*| \tilde{s}_m)= \frac{\min(p_L,p_R)-(1-p(s^*,\tilde{s}_m))}{\min(p_L,p_R)}
	\end{equation*}
	This measure is the relative reduction in error gotten by using $\tilde{s}_m$ to predict $s^*$ as compared with the $\max (p_L,p_R)$. If $\lambda(s^*| \tilde{s}_m)\le 0$, $\tilde{s}_m$ is no help in predicting $s^*$ and is discarded as a surrogate split. 
	
	Why do we need surrogate splits? For missing data and to determine the variables of importance!
	

\end{frame}

\begin{frame}{Missing Data Algorithm}
	Suppose that we have the best split $s^*$ on a node is being found. If there are missing values, the best split $s^*_m$ on $x_m$ is computed using all cases containing a valid of $x_m$ and then $s^*$ selected as that split $s^*m$ which maximizes decrease on impurity.
	
	If a case has missing values so that $s^*$ is not defined for that case. Among all non-missing variables in the case, find that one, say $x_m$ with $\tilde{s}_m$ having the highest measure of predictive association with $s^*$. The split the case using $\tilde{s}_m$.
	
	This procedure is similar to replacing a missing value in a linear model by regression on the non-missing value most highly correlated with it. 
	
\end{frame}

\begin{frame}{Variable Ranking CART}
	Let $T$ be an optimal subtree selected by the cross-validation or test sample procedure. If the Gini splitting rule has been used, then at each node compute $\Delta(\tilde{s}_m,t)= I(t)-I(t_L)-I(t_R)$.
	
	The \textbf{measure of importance of variable} $x_m$ is defined as
	\begin{equation*}
		M(x_m)= \sum_{t\in T} \Delta I (\tilde{s}_m, t)
	\end{equation*}

	The measure of important is the sum over all nodes of the decrease in impurity produced by the best split on $x_m$ at each node. Since only the relative magnitudes of the $M(x_m)$ are the interesting, the actual measures of importance we use are the normalizes quantities $100 M(x_m)/\max_m M(x_m)$. The most important variable has measure 100, and the other range 0 to 100.
\end{frame}

\begin{frame}{Trees vs. Linear Models}
	The classical linear regression assumes a relationship of the form
	\begin{equation*}
		f(X)= \theta_0 + \sum_{i=1}^p \theta_j X_j
	\end{equation*}
whereas the regression trees assume that the model has the form
\begin{equation*}
	f(X)=\sum_{i=1}^M c_m \cdot 1_{X \in R_m}
\end{equation*}
where $R_1,\ldots,R_M$ represent a partition of feature space and $1_A$ represents the indicator function (1 if $x\in A$ and 0 elsewhere)
\end{frame}

\begin{frame}{Trees vs. Linear Models (cont)}
	As we know, there is not a single model that will work all the time for any data set. 
	\begin{figure}[h]
	\centering
	\includegraphics[scale=0.4]{../../Figures/fig_trees_vs_ols.png}
\end{figure}	
\end{frame}

\begin{frame}{Trees vs. Linear Models}
	Advantages of trees are:
	\begin{itemize}
		\item They are very easy to explain even for non-expert people.
		\item Trees can easily handle a mix of predictors (quantitative, continuous) without the need of {\it dummy} variables.
	\end{itemize}
But 
\begin{itemize}
	\item Trees do not have the same level of predictive accuracy.
	\item Trees are unstable, small changes in data can cause significant chances in the final model.
\end{itemize}
\end{frame}

\begin{frame}{References}
	Materials and some of the pictures are from \citep{James2015}, \citep{genuer}, \citep{cartbook},and \citep{geron2}.
	\printbibliography 	
	
	I have used some of the graphs by hacking TiKz code from StakExchange, Inkscape for more aesthetic plots and other old tricks of \TeX
	
\end{frame}


\end{document}
