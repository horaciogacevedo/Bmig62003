\documentclass{beamer}
\DeclareFontShape{OT1}{cmss}{b}{n}{<->ssub * cmss/bx/n}{} 
\usetheme{CambridgeUS}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathbbol}
\usepackage{xcolor} % before tikz or tkz-euclide if necessary
\usepackage{tkz-euclide} % no need to load TikZ
\usepackage{multirow}
\usepackage{bm}


\usepackage[
backend=biber,
style=authoryear-icomp,
sortlocale=de_DE,
natbib=true,
url=false, 
doi=true,
eprint=false
]{biblatex}
\addbibresource{../../Bibliography/main_ML.bib}

\titlegraphic{\includegraphics[width=2cm]{../../Figures/UAMS_RGB.png}
}

\title{Statistical Machine Learning\\ Resampling Methods and Variable Selection}
\author{Horacio G\'omez-Acevedo\\ Department of Biomedical Informatics}
\begin{document}
	\begin{frame}[plain]
		\maketitle
	\end{frame}


\begin{frame}{Resampling Methods for Parameter Metrics}
	Suppose you have applied your state-of-the-art algorithm, but you don't know what is the distribution of a parameter (hyperparameter). The question is: how do I determine the bias and variance?
	
	The {\bf jackknife} and {\bf bootstrap} are {\it resampling} methodologies that help us to calculate bias and variance of arbitrary statistics (or hyperparameters), thereby improving our understanding of perfomance during classification or regression.
	
	
	 
\end{frame}

\begin{frame}{Jacknife}
	It was introduced by Maurice Quenouille around 1950's. 
	
	{\bf Motivation Example. } Let's suppose that we have 
	$m$ independent random variables $X_1,\ldots, X_m$ that follow the same distribution (not necessarily normal!). We can define the statistic $\overline{X}$ defined as $\frac{X_1 + \cdots + X_m}{m}$. The question is what is the standard deviation of this statistic given a set of observed values $X_1=x_1,\ldots, X_m=x_m$?
	
	Following the definition of variance, we can determine
	
	\begin{equation}
		\widehat{\sigma}^2(\overline{X})= \frac{1}{m(m-1)} \sum_{i=1}^m (x_i - \overline{x})^2
		\label{eq:stdev}
	\end{equation}
	
	That was simple enough, but what about calculating an estimate of the variance for other common statistics as  {\it mode}, or {\it median} or other statistics?
	
\end{frame}

\begin{frame}{Jackknife}
	Let's define the sample average of the data set deleting the jth variable as
	\begin{equation*}
		\overline{X}_{(j)}= \frac{1}{m-1} \sum_{k\ne j} X_k
	\end{equation*}
We also define the statistic that is the {\it average} of these averages
\begin{equation*}
	\overline{X}_{(\bullet)} = \frac{1}{m} \sum_{k=1}^m \overline{X}_{(k)}
\end{equation*}

The {\bf Jackknife} estimate of the standard deviation is
\begin{equation}
	\widehat{\sigma}_{Jack}^2(\overline{X})= \frac{m-1}{m} \sum_{i=1}^m (\overline{X}_{(i)} - \overline{X}_{(\bullet)})^2
\label{eq:jstdev}
\end{equation}

It can be verified that (\ref{eq:stdev}) and (\ref{eq:jstdev})  coincide; however, this process allows a generalization of this method.
\end{frame}

\begin{frame}{Jackknife}
	One of the biggest advantages of the expression (\ref{eq:jstdev}) is that when we  have an estimator $\hat{\theta}(x_1,\ldots, x_m)$ of the statistic $\theta$, we can actually estimate the variance of such estimator
	\begin{equation*}
		\hat{\sigma}_{jack}^2= \frac{m-1}{m} \sum_{i=1}^m (\hat{\theta}_{(i)}- \hat{\theta}_{(\bullet)})^2,
	\end{equation*}
where
\begin{equation*}
	\begin{split}
		\hat{\theta}_{(i)}&= \hat{\theta}(x_1, \ldots, x_{i-1},x_{i+1},\ldots, x_m) \\
		\hat{\theta}_{(\bullet)}&= \frac{1}{m-1} \sum_{i=1}^n \hat{\theta}_{(i)} 
	\end{split}
\end{equation*}

\end{frame}
		
\begin{frame}{Jackknife bias}
	It is also possible to obtain the {\bf jackknife bias} estimation
	
	Recall the definition of bias
	\begin{equation*}
		bias(\theta) = \theta - E(\hat{\theta})
	\end{equation*} 
The Jackknife estimate of bias is given by

\begin{equation*}
	bias_{jack} (\theta)= (m-1) (\hat{\theta}_{(\bullet)}- \hat{\theta})
\end{equation*}
\end{frame}		
		
\begin{frame}{Bootstrap}
	In a common definition, a {\it bootstrap} data set is one created by randomly selecting $m$ points (with replacement) from the training set $\cal{D}$.
	
	For example if our training data set consists of the points $\{(x_1,y_1),(x_2,y_2),(x_3,y_3)\}$, then a bootstrap could be 
	
	\begin{equation*}
		\begin{split}
				B_1&=\{(x_1,y_1),(x_2,y_2),(x_3,y_3)\}\\
				B_2&=\{(x_1,y_1),(x_1,y_1),(x_2,y_2)\} \\
				B_3&=\{(x_2,y_2),(x_3,y_3),(x_2,y_2)\} 
		\end{split}
	\end{equation*}
	 
\end{frame}		

\begin{frame}{Bootstrap}
	
	The bootstrap was developed by Bradley Efron in the late 1970s. 
	
	In the bootstrap setup, the data sets (say $B_j$s in our example) are treated as independent sets. The {\bf bootstrap} estimate of a statistic $\theta$ is defined as
	
	\begin{equation*}
		\hat{\theta}^{*(\bullet)}= \frac{1}{B} \sum_{b=1}^B \hat{\theta}^{*(b)},
	\end{equation*}
where $\hat{\theta}^{*(b)}$ is the estimate of $\theta$ for the sample $b$ .
\end{frame}		

\begin{frame}{Bootstrap bias and variance estimates}
	The bootstrap estimate of the bias 
	\begin{equation*}
		bias_{boot}(\theta)= \hat{\theta}^{*(\bullet)}- \hat{\theta}
	\end{equation*}
	Whereas the bootstrap estimate of the variance is
	\begin{equation*}
		\hat{\sigma}^2 (\theta)= \frac{1}{B-1}\sum_{b=1}^B \left(  \hat{\theta}^{*(b)} - \hat{\theta}^{*(\bullet)} 
		\right)^2
	\end{equation*}
\end{frame}

\begin{frame}{Bootstrap Picture}

		
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.5]{../../Figures/5.11.png}
		\end{figure}
\end{frame}


\begin{frame}{Subset Selection (Regression)}
	We have seen than when several predictors are present, it is difficult to determine which ones to keep or discard. 
	
	There are some alternatives:
	\begin{itemize}
		\item Best Subset Selection
		\item Stepwise Selection
		\begin{itemize}
			\item Forward Selection
			\item Backwards Selection
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Best Subset Selection (Regression) }
	
	Let's suppose we have a linear regression model
	\begin{equation*}
		Y = \theta_0+ \theta_1 X_1 + \cdots + \theta_p X_p + \varepsilon
	\end{equation*}
In theory, we can make (loads) of models 
\begin{equation*}
	\begin{split}
		{\cal M}_0 &: \hat{Y}= \hat{\theta}_0\\
				{\cal M}_1 &: \hat{Y}= \hat{\theta}_0+ \hat{\theta}_1 X_1 \\
				\vdots&\\
				{\cal M}_p &: \hat{Y}= \hat{\theta}_0+ \hat{\theta}_p X_p \\
				{\cal M}_{p+1} &: \hat{Y}= \hat{\theta}+ \hat{\theta_1} X_1+ \hat{\theta}_2 X_2  \\
				\vdots &\\
				{\cal M}_{2^p-1}& \colon \hat{Y }= \hat{\theta}_0+ \hat{\theta}_1 X_1 + \cdots + \hat{\theta}_p X_p 
	\end{split}
\end{equation*}
\end{frame}

\begin{frame}{Best Subset Selection}
	
	Algorithm for Best Subset Selection
	\begin{enumerate}
		\item For $k\in \{1,\ldots,p\}$:
		\begin{enumerate}
			\item Fit all $p \choose k$ models that contain exactly $k$ predictors.
			\item Pick the best among these $p \choose k$ models, and call it $\widehat{\cal M}_k$. The selection is based either by selecting the smallest RSS, or largest $R^2$.
		\end{enumerate}
	\item Select a single best model from among ${\cal M}_0$, $\widehat{\cal M}_1,\ldots, \widehat{\cal M}_p$ using cross-validated prediction error, $C_p$ (AIC), BIC, or adjusted $R^2$.
	\end{enumerate}
Notice that in step 3 we have changed our metric. If we were to proceed with the same metric (say largest $R^2$), we will end up with the model including all parameters since $R^2$ increases monotonically towards 1 as the number of predictors increases. 
\end{frame}

\begin{frame}{Forward Stepwise Selection}
	The algorithm for this problem is stated as
	\begin{enumerate}
		\item For $k=\{0,\ldots, p-1\}$:
		\begin{enumerate}
			\item Consider all $p-k$ models that augment the predictors in $\widehat{\cal M}_k$ with one additional predictor.
			\item Choose the {\it best} among these $p-k$ models and call it $\widehat{\cal M}_{k+1}$. The metric is defined as having the smallest RSS or highest $R^2$.
		\end{enumerate}
	\item Select a single best model from among ${\cal M}_0,\widehat{\cal M}_1,\ldots, \widehat{\cal M}_p$ using cross-validated prediction error $C_p$ (AIC), BIC, or adjusted $R^2$.
		\end{enumerate}
	So, instead of comparing $2^p$ models, we will be comparing $1 + \frac{p(p+1)}{2}$ models. 
	
	Note that the forward stepwise tends to do well in practice, it is not guaranteed to find the best possible model!

\end{frame}

\begin{frame}{Forward Stepwise Selection}
	
	 \begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../../Figures/fig_forward.png}
	\end{figure}
\end{frame}

\begin{frame}{Backwards Stepwise Selection}
	It requires the same number of steps as the forward selection. The algorithm runs as follows:
	
	\begin{enumerate}
		\item Let $\widehat{\cal M}_p$  denote the full model with all predictors.
		\item For $k=p,p-1,\ldots,1$:
		\begin{itemize}
			\item Consideer all $k$ models that contain all but one of the predictors in $\widehat{\cal M}_k$, for a total of $k-1$ predictors.
			\item Choose the {\it best} among these $k$ models, and call it $\widehat{\cal M}_{k-1}$. Again, we consider one of the metrics such as the smallest RSS or largest $R^2$.
		\end{itemize}
	 \item Select a single best model from among ${\cal M}_0,\widehat{\cal M}_1, \ldots, \widehat{\cal M}_p$ using cross-validated prediction error, $C_p$ (AIC), BIC or adjusted $R^2$.
	\end{enumerate}
\end{frame}

\begin{frame}{Backwards Stepwise Selection}
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../../Figures/fig_backwards.png}
	\end{figure}
\end{frame}

\begin{frame}{Selection criteria}
	Let's suppose we have fitted a model containing $d$ predictor, the $C_p$ estimate of test MSE is computed using the equation
	\begin{equation*}
		C_p= \frac{1}{n} (RSS+ 2d \hat{\sigma}^2),
	\end{equation*}
	where $\hat{\sigma}^2$ is an estimate of the variance of the error $\varepsilon$ associated with each response measurement. $C_p$ is an unbiased estimate of test MSE. Thus, we choose the model with the lowest $C_p$ value. 
	
	The Akaike information criteria (AIC) is defined for a large class of models fit by maximum likelihood. 
	\begin{equation*}
		AIC \approx \frac{1}{n \hat{\sigma}^2} (RSS+ 2d \hat{\sigma}^2)
	\end{equation*}
 The Bayesian information criteria (BIC) is derived from a Bayesian point of view
 \begin{equation*}
 	BIC \approx \frac{1}{n} (RSS+ \log(n)d \hat{\sigma}^2)
 \end{equation*}

\end{frame}

\begin{frame}{Adjusted $R^2$}
	Finally, the so-called {\bf adjusted} $R^2$ statistic is defined as
	\begin{equation*}
		\textrm{Adjusted }R^2= 1 - \frac{RSS/(n-d-1)}{TSS/(n-1)}= 1 - \frac{n-1}{n-d-1} \cdot \frac{RSS}{TSS}
	\end{equation*}
Recall that $R^2= 1 - RSS/TSS$ where $TSS= \sum (y_i - \overline{y})^2$ is the total sum of squares. 
 So adding the adjusted $R^2$ statistic pays a price for the inclusion of unnecessary variables in the model. 

\end{frame}

\begin{frame}{Optimal Selection}
	
	
 \begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{../../Figures/fig_cp.png}
\end{figure}
\end{frame}


\begin{frame}{Final Thoughts}
	
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{../../Figures/fig_efron_paper.png}
	\end{figure}
\end{frame}
		
\begin{frame}{What we have learned?	}
	\begin{itemize}
		\item The power of resampling methods for estimation of parameters.
		\item Bootstrap is considered one of the most powerful statistical techniques for estimation. 
		\item Methods for subset selection of covariates in a regression setting.
		\item We explore the forward and backward selection.
	\end{itemize}
\end{frame}

\begin{frame}{References}
	Materials and some of the pictures are from \citep{James2015}, \citep{hastie01}, and \citep{geron2}. Main reference for resampling are \citep{efronbook} and \citep{efron2012}.
	\printbibliography 	
	
	I have used some of the graphs by hacking TiKz code from StakExchange, Inkscape for more aesthetic plots and other old tricks of \TeX
	
\end{frame}



	
\end{document}